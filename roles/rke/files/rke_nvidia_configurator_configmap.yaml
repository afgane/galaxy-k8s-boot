apiVersion: v1
kind: ConfigMap
metadata:
  name: rke-nvidia-configurator-configmap
  namespace: gpu-operator
data:
  config.toml.tmpl: |
    [plugins.opt]
      path = "{{ .NodeConfig.Containerd.Opt }}"
    [plugins.cri]
      stream_server_address = "127.0.0.1"
      stream_server_port = "10010"
      enable_selinux = {{ .NodeConfig.SELinux }}
    {{- if .IsRunningInUserNS }}
      disable_cgroup = true
      disable_apparmor = true
      restrict_oom_score_adj = true
    {{end}}
    {{- if .NodeConfig.AgentConfig.PauseImage }}
      sandbox_image = "{{ .NodeConfig.AgentConfig.PauseImage }}"
    {{end}}
    {{- if .NodeConfig.AgentConfig.Snapshotter }}
    [plugins.cri.containerd]
      disable_snapshot_annotations = true
      snapshotter = "{{ .NodeConfig.AgentConfig.Snapshotter }}"
    {{end}}
    {{- if not .NodeConfig.NoFlannel }}
    [plugins.cri.cni]
      bin_dir = "{{ .NodeConfig.AgentConfig.CNIBinDir }}"
      conf_dir = "{{ .NodeConfig.AgentConfig.CNIConfDir }}"
    {{end}}
    [plugins.cri.containerd.runtimes.runc]
      # ---- changed from 'io.containerd.runc.v2' for GPU support
      runtime_type = "io.containerd.runtime.v1.linux"
    # ---- added for GPU support
    [plugins.cri.containerd.runtimes.runc.options]
      Runtime = "/usr/local/nvidia/toolkit/nvidia-container-runtime"
    [plugins.cri.containerd.runtimes.nvidia]
      runtime_type = "io.containerd.runc.v2"
    [plugins.cri.containerd.runtimes.nvidia.options]
      Runtime = "/usr/local/nvidia/toolkit/nvidia-container-runtime"
    # ---- end added for GPU support
    {{ if .PrivateRegistryConfig }}
    {{ if .PrivateRegistryConfig.Mirrors }}
    [plugins.cri.registry.mirrors]{{end}}
    {{range $k, $v := .PrivateRegistryConfig.Mirrors }}
    [plugins.cri.registry.mirrors."{{$k}}"]
      endpoint = [{{range $i, $j := $v.Endpoints}}{{if $i}}, {{end}}{{printf "%q" .}}{{end}}]
    {{end}}
    {{range $k, $v := .PrivateRegistryConfig.Configs }}
    {{ if $v.Auth }}
    [plugins.cri.registry.configs."{{$k}}".auth]
      {{ if $v.Auth.Username }}username = {{ printf "%q" $v.Auth.Username }}{{end}}
      {{ if $v.Auth.Password }}password = {{ printf "%q" $v.Auth.Password }}{{end}}
      {{ if $v.Auth.Auth }}auth = {{ printf "%q" $v.Auth.Auth }}{{end}}
      {{ if $v.Auth.IdentityToken }}identitytoken = {{ printf "%q" $v.Auth.IdentityToken }}{{end}}
    {{end}}
    {{ if $v.TLS }}
    [plugins.cri.registry.configs."{{$k}}".tls]
      {{ if $v.TLS.CAFile }}ca_file = "{{ $v.TLS.CAFile }}"{{end}}
      {{ if $v.TLS.CertFile }}cert_file = "{{ $v.TLS.CertFile }}"{{end}}
      {{ if $v.TLS.KeyFile }}key_file = "{{ $v.TLS.KeyFile }}"{{end}}
      {{ if $v.TLS.InsecureSkipVerify }}insecure_skip_verify = true{{end}}
    {{end}}
    {{end}}
    {{end}}
  configurator.sh: |
    echo Node name is $NODE_NAME
    export TOOLKIT_POD=$(kubectl get -n gpu-operator-resources pods -l app=nvidia-container-toolkit-daemonset --field-selector=spec.nodeName=$NODE_NAME -o=name)
    echo Toolkit pod is $TOOLKIT_POD
    until kubectl logs --tail 10 -n gpu-operator-resources $TOOLKIT_POD | grep "Waiting for signal";
    do echo Waiting for toolkit to be ready;
    sleep 2;
    done;
    echo Restarting containerd with new rke2 nvidia config;
    cp /src/config.toml.tmpl /dest/config.toml.tmpl
    echo Restarting nvidia toolkit daemonset
    kubectl rollout restart -n gpu-operator-resources daemonset/nvidia-container-toolkit-daemonset
    echo Sleeping for 30 seconds
    sleep 30
    echo Restarting remaining nvidia daemonsets
    kubectl rollout restart -n gpu-operator-resources daemonset/nvidia-dcgm-exporter
    kubectl rollout restart -n gpu-operator-resources daemonset/gpu-feature-discovery
    kubectl rollout restart -n gpu-operator-resources daemonset/nvidia-device-plugin-daemonset
